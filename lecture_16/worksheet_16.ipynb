{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 16\n",
    "\n",
    "Name:  shengwen ouyang\n",
    "UID: U01208928\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Support Vector Machines (Non-linear case)\n",
    "\n",
    "## Support Vector Machines\n",
    "\n",
    "Follow along in class to implement the perceptron algorithm and create an animation of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) As we saw in class, the form\n",
    "$$w^T x + b = 0$$\n",
    "while simple, does not expose the inner product `<x_i, x_j>` which we know `w` depends on, having done the math. This is critical to applying the \"kernel trick\" which allows for learning non-linear decision boundaries. Let's modify the above algorithm to use the form\n",
    "$$\\sum_i \\alpha_i <x_i, x> + b = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "CENTERS = [[0, 1], [1, 0]]\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = .05\n",
    "expanding_rate = .99\n",
    "retracting_rate = 1.1\n",
    "\n",
    "X, labels = datasets.make_blobs(n_samples=10, centers=CENTERS, cluster_std=0.2, random_state=0)\n",
    "Y = np.array(list(map(lambda x : -1 if x == 0 else 1, labels.tolist())))\n",
    "\n",
    "alpha_i = np.zeros((len(X),))\n",
    "b = 0\n",
    "\n",
    "def snap(x, alpha_i, b, error, X, Y, gamma):\n",
    "    # create a mesh to plot in\n",
    "    h = .01  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    meshData = np.c_[xx.ravel(), yy.ravel()]\n",
    "    cs = np.array([x for x in 'gb'])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,0],X[:,1],color=cs[labels].tolist(), s=50, alpha=0.8)\n",
    "\n",
    "    if error:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .12, color='r',fill=False))\n",
    "    else:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .12, color='y',fill=False))\n",
    "   \n",
    "    Z = predict_many(alpha_i, b, meshData, X, Y, gamma)\n",
    "    Z = np.array([0 if z <=0 else 1 for z in Z]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=.5, cmap=plt.cm.Paired)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "def predict(alpha_i, b, x, X, Y, gamma):\n",
    "    return sum(alpha_i[i] * Y[i] * rbf_kernel(X[i], x, gamma) for i in range(len(X))) + b\n",
    "\n",
    "\n",
    "def predict_many(alpha_i, b, Z, X, Y, gamma):\n",
    "    # This will use the RBF kernel for predictions.\n",
    "    return np.array([predict(alpha_i, b, z, X, Y, gamma) for z in Z])\n",
    "gamma = 0.5\n",
    "images = []\n",
    "for epoch in range(epochs):\n",
    "    # pick a point from X at random\n",
    "    i = np.random.randint(0, len(X))\n",
    "    error = False\n",
    "    x, y = X[i], Y[i]\n",
    "    \n",
    "    # Calculate the prediction for the current point\n",
    "    prediction = predict(alpha_i, b, x, X, Y, gamma)\n",
    "    \n",
    "    # Check if the prediction is correct\n",
    "    if y * prediction <= 0:  # Misclassified point\n",
    "        error = True\n",
    "        # Update rule for the alpha coefficients and bias term\n",
    "        alpha_i[i] += learning_rate * y\n",
    "        b += learning_rate * y\n",
    "    else:\n",
    "        # If no error, let's try to marginally adjust the alphas downwards\n",
    "        # to ensure we're not overfitting to the training data\n",
    "        alpha_i[i] = max(0, alpha_i[i] * expanding_rate)\n",
    "        b *= expanding_rate\n",
    "    \n",
    "    # Snap a picture of the current state\n",
    "    images.append(snap(x, alpha_i, b, error, X, Y, gamma))\n",
    "\n",
    "\n",
    "images[0].save(\n",
    "    'svm_dual.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a configurable kernel function to apply in lieu of the dot product. Try it out on a dataset that is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "def rbf_kernel(x_i, x_j, gamma):\n",
    "    return np.exp(-gamma * np.linalg.norm(x_i - x_j) ** 2)\n",
    "\n",
    "# Your dataset creation and initial variables remain the same\n",
    "def predict(alpha_i, b, x, X, Y, gamma):\n",
    "    return sum(alpha_i[i] * Y[i] * rbf_kernel(X[i], x, gamma) for i in range(len(X))) + b\n",
    "\n",
    "\n",
    "def predict_many(alpha_i, b, Z, X, Y, gamma):\n",
    "    # This will use the RBF kernel for predictions.\n",
    "    return np.array([predict(alpha_i, b, z, X, Y, gamma) for z in Z])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume we fit an SVM using a polynomial Kernel function and it seems to overfit the data. How would you adjust the tuning parameter `n` of the kernel function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the degree n of the polynomial kernel in an SVM is a powerful method to control model complexity and combat overfitting. It's essential to approach this adjustment methodically, using techniques like cross-validation to find the optimal balance between underfitting and overfitting. Remember, the goal is to achieve a model that generalizes well to unseen data, not one that performs exceptionally on the training data but poorly on new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Assume we fit an SVM using a RBF Kernel function and it seems to underfit the data. How would you adjust the tuning parameter `sigma` of the kernel function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the σ parameter of the RBF kernel is a key method for addressing underfitting in SVM models. A larger σ increases the model's capacity to capture more complex patterns by allowing points to influence each other over greater distances. However, it's important to balance this adjustment with considerations of the regularization parameter and feature scaling to ensure optimal model performance. Always use systematic tuning and validation techniques like cross-validation to determine the best parameters for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Tune the parameter of a specific Kernel function, to fit an SVM (using your code above) to the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data = np.loadtxt(\"spiral.data\")\n",
    "X, y = data[:, :2], data[:, 2]\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title('Spiral Data')\n",
    "plt.show()\n",
    "\n",
    "clf = SVC(kernel='rbf')\n",
    "\n",
    "gamma_range = [0.1, 0.5, 1, 2, 5, 10]\n",
    "\n",
    "\n",
    "best_score = 0\n",
    "best_gamma = 0\n",
    "\n",
    "for gamma in gamma_range:\n",
    "    clf.set_params(gamma=gamma)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    score = clf.score(X, y)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_gamma = gamma\n",
    "\n",
    "print(f'Best Gamma: {best_gamma}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "76ca05dc3ea24b2e3b98cdb7774adfbb40773424bf5109b477fd793f623715af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
